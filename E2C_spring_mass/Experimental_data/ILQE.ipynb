{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import sympy as sp\n",
    "\n",
    "class ilqr:\n",
    "    def __init__(self,init_state,target_state,initial_guess,dt,start_time,end_time,f_disc,A,B,Q_k,R_k,Q_T,parameters,n_iterations,traj):\n",
    "        self.init_state_ = init_state\n",
    "        self.target_state_ = target_state\n",
    "        self.inputs_ = initial_guess\n",
    "        self.n_states_ = np.shape(init_state)[0]\n",
    "        self.n_inputs_ = np.shape(initial_guess)[1]\n",
    "        self.traj_ = traj\n",
    "\n",
    "        self.dt_ = dt\n",
    "        self.start_time_ = start_time\n",
    "        self.end_time_ = end_time\n",
    "        self.time_span_ = np.arange(start_time, end_time, dt).flatten()\n",
    "        self.n_timesteps_ = np.shape(self.time_span_)[0]\n",
    "        # Dynamics\n",
    "        self.f_ = f_disc\n",
    "        self.A_ = A\n",
    "        self.B_ = B\n",
    "        # Weighting\n",
    "        self.Q_k_ = Q_k\n",
    "        self.R_k_ = R_k\n",
    "        self.Q_T_ = Q_T\n",
    "        self.parameters_ = parameters\n",
    "\n",
    "        # Max iterations\n",
    "        self.n_iterations_ = n_iterations\n",
    "\n",
    "    def rollout(self):\n",
    "        states = np.zeros((self.n_timesteps_ + 1, self.n_states_))\n",
    "        inputs = np.zeros((self.n_timesteps_, self.n_inputs_))\n",
    "        current_state = self.init_state_\n",
    "\n",
    "        for ii in range(0,self.n_timesteps_):\n",
    "            current_input = self.inputs_[ii,:]\n",
    "            next_state = self.f_(current_state, current_input, self.dt_, self.parameters_).flatten()\n",
    "            #next_state = self.f_(current_state, current_input, self.dt_, self.parameters_).flatten()\n",
    "            # Store states and inputs\n",
    "            states[ii + 1,:] = next_state\n",
    "            inputs[ii,:] = current_input # in case we have a control law, we store the input used\n",
    "            # Update the current state\n",
    "            current_state = next_state\n",
    "\n",
    "        # Store the trajectory(states, inputs)\n",
    "        self.states_ = states\n",
    "        self.inputs_ = inputs\n",
    "        return states, inputs\n",
    "\n",
    "    def compute_cost(self,states,inputs):\n",
    "        # Initialize cost\n",
    "        total_cost = 0.0\n",
    "        for ii in range(0,self.n_timesteps_):\n",
    "            current_x = states[ii,:].flatten() \n",
    "            current_u = inputs[ii,:].flatten()\n",
    "            traj_diff= (self.traj_state_[ii,:].flatten()-current_x)\n",
    "            current_cost =  traj_diff.T@self.Q_T_@traj_diff\n",
    "            current_cost += current_u.T@self.R_k_@current_u \n",
    "            total_cost = total_cost+current_cost\n",
    "        # Compute terminal cost\n",
    "        #terminal_difference = (self.target_state_-states[-1,:]).flatten()\n",
    "        #terminal_cost = terminal_difference.T@self.Q_T_@terminal_difference\n",
    "        #total_cost = total_cost+terminal_cost\n",
    "        return total_cost\n",
    "\n",
    "    def backwards_pass(self):\n",
    "        # First compute initial conditions (end boundary condition)\n",
    "        # Value function hessian and gradient\n",
    "        V_xx = self.Q_T_\n",
    "        end_difference = (self.states_[-1, :] - self.target_state_).flatten()\n",
    "        end_difference = end_difference.flatten()  # Make sure its the right dimension\n",
    "        V_x = self.Q_T_@end_difference\n",
    "\n",
    "        # Initialize storage variables\n",
    "        k_trj = np.zeros((self.n_timesteps_,self.n_inputs_))\n",
    "        K_trj = np.zeros((self.n_timesteps_,self.n_inputs_,self.n_states_))\n",
    "\n",
    "        # Initialize cost reduction\n",
    "        expected_cost_reduction = 0\n",
    "        expected_cost_reduction_grad = 0\n",
    "        expected_cost_reduction_hess = 0\n",
    "\n",
    "        # for loop backwards in time\n",
    "        for idx in reversed(range(0, self.n_timesteps_)):\n",
    "            # Grab the current variables in the trajectory\n",
    "            current_x = self.states_[idx,:]\n",
    "            current_u = self.inputs_[idx,:]\n",
    "\n",
    "            # R_k_updated\n",
    "            # Define the expansion coefficients and the loss gradients\n",
    "            l_xx = self.Q_k_ # For now zeros, can add in a target to track later on\n",
    "            l_uu = self.R_k_\n",
    "\n",
    "            l_x = self.Q_k_@np.zeros(self.n_states_).flatten() # For now zeros, can add in a target to track later on\n",
    "            l_u = self.R_k_@(current_u).flatten()\n",
    "\n",
    "            # Get the jacobian of the discretized dynamics\n",
    "            A_k = self.A_(current_x, current_u, self.dt_, self.parameters_)\n",
    "            B_k = self.B_(current_x, current_u, self.dt_, self.parameters_)\n",
    "\n",
    "            Q_x = l_x + A_k.T@V_x\n",
    "            Q_u = l_u+B_k.T@V_x\n",
    "            Q_ux = B_k.T@V_xx@A_k\n",
    "            Q_uu = l_uu + B_k.T@V_xx@B_k\n",
    "            Q_xx = l_xx+A_k.T@V_xx@A_k\n",
    "\n",
    "            # Compute gains\n",
    "            Q_uu_inv = np.linalg.inv(Q_uu) # This can sometimes go singular\n",
    "            k = -Q_uu_inv@Q_u\n",
    "            K = -Q_uu_inv@Q_ux\n",
    "\n",
    "            # Store gains\n",
    "            k_trj[idx,:] = k\n",
    "            K_trj[idx,:,:] = K\n",
    "\n",
    "            # Update the expected reduction\n",
    "            current_cost_reduction_grad = -Q_u.T@k\n",
    "            current_cost_reduction_hess = 0.5 * k.T @ (Q_uu) @ (k)\n",
    "            current_cost_reduction = current_cost_reduction_grad + current_cost_reduction_hess\n",
    "\n",
    "            expected_cost_reduction_grad +=  current_cost_reduction_grad\n",
    "            expected_cost_reduction_hess +=  current_cost_reduction_hess\n",
    "            expected_cost_reduction += + current_cost_reduction\n",
    "\n",
    "            # Update hessian and gradient for value function (If we arent using regularization we can simplify this computation)\n",
    "            V_x = Q_x +K.T@Q_uu@k +K.T@Q_u + Q_ux.T@k\n",
    "            V_xx = (Q_xx+Q_ux.T@K+K.T@Q_ux+K.T@Q_uu@K)\n",
    "\n",
    "        # Store expected cost reductions\n",
    "        self.expected_cost_reduction_grad_ = expected_cost_reduction_grad\n",
    "        self.expected_cost_reduction_hess_ = expected_cost_reduction_hess\n",
    "        self.expected_cost_reduction_ = expected_cost_reduction\n",
    "\n",
    "        # Store gain schedule\n",
    "        self.k_feedforward_ = k_trj\n",
    "        self.K_feedback_ = K_trj\n",
    "        return (k_trj,K_trj,expected_cost_reduction)\n",
    "\n",
    "    def forwards_pass(self, learning_rate):\n",
    "        states = np.zeros((self.n_timesteps_ + 1, self.n_states_))\n",
    "        inputs = np.zeros((self.n_timesteps_, self.n_inputs_))\n",
    "        current_state = self.init_state_\n",
    "\n",
    "        # set the first state to be  the initial\n",
    "        states[1,:] = current_state\n",
    "        for ii in range(0,self.n_timesteps_):\n",
    "            # Get the current gains and compute the feedforward and feedback terms\n",
    "            current_feedforward = learning_rate * self.k_feedforward_[ii,:]\n",
    "            current_feedback = self.K_feedback_[ii,:,:]@(current_state - self.states_[ii,:])\n",
    "            current_input = self.inputs_[ii,:] + current_feedback + current_feedforward\n",
    "\n",
    "            # simulate forward\n",
    "            next_state = self.f_(current_state, current_input, self.dt_, self.parameters_).flatten()\n",
    "            # Store states and inputs\n",
    "            states[ii + 1,:] = next_state\n",
    "            inputs[ii,:] = current_input.flatten()\n",
    "\n",
    "            # Update the current state\n",
    "            current_state = next_state\n",
    "        return (states,inputs)\n",
    "    def solve(self):\n",
    "        # Compute the rollout to get the initial trajectory with the\n",
    "        # initial guess\n",
    "        [states,inputs] = self.rollout()\n",
    "        # Compute the current cost of the initial trajectory\n",
    "        current_cost = self.compute_cost(states,inputs)\n",
    "        \n",
    "        learning_speed = 0.95 # This can be modified, 0.95 is very slow\n",
    "        low_learning_rate = 0.05 # if learning rate drops to this value stop the optimization\n",
    "        low_expected_reduction = 1e-3 # Determines optimality\n",
    "        armijo_threshold = 0.1 # Determines if current line search solve is good (this is typically labeled as \"c\")\n",
    "        for ii in range(0,self.n_iterations_):\n",
    "            print('Starting iteration: ',ii,', Current cost: ',current_cost)\n",
    "            # Compute the backwards pass\n",
    "            (k_feedforward,K_feedback,expected_reduction) = self.backwards_pass()\n",
    "            print('Expected cost reduction: ',expected_reduction)\n",
    "            if(abs(expected_reduction)<low_expected_reduction):\n",
    "                # If the expected reduction is low, then end the\n",
    "                # optimization\n",
    "                print(\"Stopping optimization, optimal trajectory\")\n",
    "                break\n",
    "            learning_rate = 1\n",
    "            armijo_flag = 0\n",
    "            # Execute linesearch until the armijo condition is met (for\n",
    "            # now just check if the cost decreased) TODO add real\n",
    "            # armijo condition\n",
    "            while(learning_rate > 0.05 and armijo_flag == 0):\n",
    "                # Compute forward pass\n",
    "                (new_states,new_inputs)=self.forwards_pass(learning_rate)\n",
    "                new_cost = self.compute_cost(new_states,new_inputs)\n",
    "\n",
    "                # Calculate armijo condition\n",
    "                cost_difference = (current_cost - new_cost)\n",
    "                expected_cost_redu = learning_rate*self.expected_cost_reduction_grad_ + learning_rate*learning_rate*self.expected_cost_reduction_hess_\n",
    "                armijo_flag = cost_difference/expected_cost_redu > armijo_threshold\n",
    "                if(armijo_flag == 1):\n",
    "                    # Accept the new trajectory if armijo condition is\n",
    "                    # met\n",
    "                    current_cost = new_cost\n",
    "                    self.states_ = new_states\n",
    "                    self.inputs_ = new_inputs\n",
    "                else:\n",
    "                    # If no improvement, decrease the learning rate\n",
    "                    learning_rate = learning_speed*learning_rate\n",
    "                    # print('Reducing learning rate to: ',learning_rate)\n",
    "            if(learning_rate<low_learning_rate):\n",
    "                # If learning rate is low, then stop optimization\n",
    "                print(\"Stopping optimization, low learning rate\")\n",
    "                break\n",
    "        # Return the current trajectory\n",
    "        states = self.states_\n",
    "        inputs = self.inputs_\n",
    "        return states,inputs,k_feedforward,K_feedback,current_cost\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy.matrices import Matrix\n",
    "\n",
    "def symbolic_food_system(A,B):\n",
    "        # Define the states and inputs\n",
    "    x_1,x_2,x_3,x_4,u,dt = sp.symbols('x_1 x_2 x_3 x_4 u dt')\n",
    "    inputs = Matrix([u])\n",
    "    states = Matrix([x_1,x_2,x_3,x_4])\n",
    "    # Defining the dynamics of the system\n",
    "    x = [x_1,x_2,x_3,x_4]\n",
    "    u = [u]\n",
    "    x_vec = sp.Matrix(x)\n",
    "    u_vec = sp.Matrix(u)\n",
    "\n",
    "    # Define the system dynamics\n",
    "    A=Matrix([[A[0,0],A[0,1],A[0,2],A[0,3]],[A[1,0],A[1,1],A[1,2],A[1,3]],[A[2,0],A[2,1],A[2,2],A[2,3]],[A[3,0],A[3,1],A[3,2],A[3,3]]])\n",
    "    B=Matrix([[B[0]],[B[1]],[B[2]],[B[3]]])\n",
    "    f = A @ x_vec + B @ u_vec\n",
    "\n",
    "    # Discretize the dynamics using Euler integration\n",
    "    f_disc = x_vec + dt * f\n",
    "\n",
    "    # Compute the Jacobians with respect to states and inputs\n",
    "    A_disc = f_disc.jacobian(x_vec)\n",
    "    B_disc = f_disc.jacobian(u_vec)\n",
    "\n",
    "    # Convert the symbolic expressions to callable functions\n",
    "    f_disc_func = sp.lambdify((x_vec, u_vec, dt), f_disc)\n",
    "    A_disc_func = sp.lambdify((x_vec, u_vec, dt), A_disc)\n",
    "    B_disc_func = sp.lambdify((x_vec, u_vec, dt), B_disc)\n",
    "\n",
    "    return f_disc_func, A_disc_func, B_disc_func\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "A=torch.load('/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/latent_2/control_data_2.pt')\n",
    "A=A[:100]\n",
    "states=[]\n",
    "A_mat=[]\n",
    "B_mat=[]\n",
    "for i in range(len(A)):\n",
    "    states.append(A[i][0].detach().numpy()[:4])\n",
    "    A_mat.append(A[i][1].detach().numpy()[:4,:4])\n",
    "    B_mat.append(A[i][2].detach().numpy()[:4,:1].squeeze())\n",
    "A=A_mat[1]\n",
    "B=B_mat[1]\n",
    "\n",
    "\n",
    "R = np.array([[0.001]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16299967,  0.09749826,  0.02166789,  0.11004608])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_lambdifygenerated() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m ilqr_ \u001b[39m=\u001b[39m ilqr(init_state,target_state,initial_guess,dt,start_time,end_time,f,A,B,Q_k,R_k,Q_T,parameters,n_iterations,states)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Solve for swing up\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m (states,inputs,k_feedforward,K_feedback,current_cost) \u001b[39m=\u001b[39m ilqr_\u001b[39m.\u001b[39;49msolve()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Animate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m animate_pendulum(states,inputs,dt,parameters)\n",
      "\u001b[1;32m/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     \u001b[39m# Compute the rollout to get the initial trajectory with the\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     \u001b[39m# initial guess\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m     [states,inputs] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m     \u001b[39m# Compute the current cost of the initial trajectory\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m     current_cost \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_cost(states,inputs)\n",
      "\u001b[1;32m/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m ii \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_timesteps_):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     current_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs_[ii,:]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     next_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf_(current_state, current_input, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdt_, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters_)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Store states and inputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/avi/Desktop/Food_GVAE-master/E2C_spring_mass/Experimental_data/ILQE.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     states[ii \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,:] \u001b[39m=\u001b[39m next_state\n",
      "\u001b[0;31mTypeError\u001b[0m: _lambdifygenerated() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import dynamics\n",
    "(f,A,B) = symbolic_food_system(A,B)\n",
    "\n",
    "# Initialize timings\n",
    "dt = 0.1\n",
    "start_time = 0\n",
    "end_time = 10\n",
    "time_span = np.arange(start_time, end_time, dt).flatten()\n",
    "\n",
    "# Set desired state\n",
    "n_states = 4\n",
    "n_inputs = 1\n",
    "init_state = np.array([0,0,0,0])   # Initial state\n",
    "target_state = states[-1]  # Target state\n",
    "\n",
    "# Initial guess of zeros, but you can change it to any guess\n",
    "initial_guess = 0.1*np.ones((np.shape(time_span)[0],n_inputs))\n",
    "# Define weighting matrices\n",
    "Q_k = np.zeros((n_states,n_states)) # zero weight to penalties along a strajectory since we are finding a trajectory\n",
    "R_k = 0.001*np.eye(n_inputs)\n",
    "\n",
    "# Set the terminal cost\n",
    "Q_T = 100*np.eye(n_states)\n",
    "\n",
    "parameters =np.array([0.1,0.1,0.1,0.1])\n",
    "# Specify max number of iterations\n",
    "n_iterations = 100\n",
    "\n",
    "# Initialize ilqr object\n",
    "ilqr_ = ilqr(init_state,target_state,initial_guess,dt,start_time,end_time,f,A,B,Q_k,R_k,Q_T,parameters,n_iterations,states)\n",
    "\n",
    "# Solve for swing up\n",
    "(states,inputs,k_feedforward,K_feedback,current_cost) = ilqr_.solve()\n",
    "\n",
    "# Animate\n",
    "animate_pendulum(states,inputs,dt,parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
